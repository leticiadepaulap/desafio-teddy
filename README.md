# Desafio TÃ©cnico â€“ Engenheiro de Dados | Teddy Open Finance  
**Pipeline de Dados com Airflow e PostgreSQL**

Este projeto implementa um pipeline de dados automatizado utilizando **Apache Airflow**, que coleta dados de uma API pÃºblica, transforma e carrega apenas registros completos no **Data Lake**, armazenado no schema `Teddy_360` de um banco **PostgreSQL**.

---

## ğŸ“¸ VisÃ£o Geral

### DAG no Airflow  
> Imagem da DAG executada com sucesso

![DAG no Airflow](images/dag_airflow.png)

### Tabela no Banco  
> Estrutura da tabela `tarefas` no schema `Teddy_360`

![Tabela no Postgres](images/tabela_tarefas.png)

---

## ğŸ¯ Objetivos do Projeto

- Coletar os dados da API e armazenÃ¡-los no Data Lake, no schema `Teddy_360` do banco de dados PostgreSQL.
- Garantir que apenas linhas de dados **completas** sejam armazenadas. Linhas incompletas sÃ£o excluÃ­das.
- Criar a string de conexÃ£o da engine utilizando SQLAlchemy.
- Criar o script SQL para estruturaÃ§Ã£o da tabela.
- Realizar o versionamento com Git, mantendo organizaÃ§Ã£o e semÃ¢ntica nos commits.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- Apache Airflow
- PostgreSQL
- Python 3.12
- Docker
- Bibliotecas:
  - `requests`
  - `pandas`
  - `SQLAlchemy`
  - `python-dotenv`
  - `psycopg2-binary`
  - `pytest` (para testes unitÃ¡rios)

---

## ğŸ³ Como Rodar com Docker

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/desafio-teddy.git
cd desafio-teddy

# Para este case, o arquivo `.env` foi mantido localmente e estÃ¡ ignorado via `.gitignore`.

# Suba os containers com Docker Compose
docker-compose up --build
```
Acesse o Airflow em: http://localhost:8080

## ğŸ§ª Como Rodar os Testes

### Ative a virtualenv
source venv/bin/activate

### Instale as dependÃªncias
pip install -r requirements.txt

### Rode os testes
pytest
Todos os testes estÃ£o localizados na pasta tests/ e cobrem os mÃ³dulos de extraÃ§Ã£o, transformaÃ§Ã£o e carga de dados.

---

## ğŸ“ Estrutura do Projeto
```bash
CÃ³digo
desafio-teddy/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ api_data_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ loading.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.json
â”‚   â””â”€â”€ filtered_data.json
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ dag_airflow.png
â”‚   â””â”€â”€ tabela_tarefas.png
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ—ï¸ Arquitetura e EstratÃ©gia de Dados
O pipeline foi desenvolvido com foco em simplicidade, modularidade e escalabilidade, respeitando boas prÃ¡ticas de engenharia de dados.

Arquitetura
OrquestraÃ§Ã£o com Apache Airflow: cada etapa do pipeline (extraÃ§Ã£o, transformaÃ§Ã£o e carga) Ã© representada como uma task independente, permitindo rastreabilidade e reprocessamento.

Armazenamento em Data Lake PostgreSQL: os dados sÃ£o persistidos no schema Teddy_360, isolando o contexto do projeto e facilitando futuras anÃ¡lises.

ModularizaÃ§Ã£o em Python: os scripts foram separados em mÃ³dulos (extract.py, transform.py, loading.py) para facilitar manutenÃ§Ã£o e testes.

Testes automatizados com Pytest: cada mÃ³dulo possui testes unitÃ¡rios que garantem a integridade das funÃ§Ãµes principais.

EstratÃ©gia de Dados
Filtragem de registros incompletos: o campo completed foi utilizado como critÃ©rio de integridade. Registros com completed = False foram considerados incompletos e excluÃ­dos na etapa de transformaÃ§Ã£o.

Evita duplicaÃ§Ãµes: antes de inserir dados no banco, o pipeline verifica se o id da tarefa jÃ¡ existe, garantindo que apenas dados novos sejam carregados.

Versionamento com timestamp: cada registro inserido recebe um campo updated_at, permitindo rastrear atualizaÃ§Ãµes e facilitar auditorias.

Uso de variÃ¡veis de ambiente: a conexÃ£o com o banco Ã© configurada via .env, promovendo seguranÃ§a e flexibilidade entre ambientes.